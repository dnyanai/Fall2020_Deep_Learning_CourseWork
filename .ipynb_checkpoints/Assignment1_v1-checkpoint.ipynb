{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Assignment 1 \n",
    "\n",
    "### Group 2 : Apurva Shekhar | Dnaynai Surkutwar | Ritu Ranjani Ravishankar | Suchita Negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "1. The process of sampling from a discrete distribution with 2 outcomes was described on Page 24 of Lecture 2.\n",
    " Describe the sampling algorithm that is able to sample from a discrete distribution with K outcomes.\n",
    " Write a Python program to generate 100 samples from the following distribution:\n",
    "    P(X = 1) = 0.7\n",
    "    P(X = 2) = 0.2\n",
    "    P(X = 3) = 0.1\n",
    " Count the number of each outcome. Are they close in number to what the distribution predicts?\n",
    " \n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Python Program:\n",
    "from random import random\n",
    "import pprint\n",
    "\n",
    "## Function to compute the distribution\n",
    "def generate_distribution():\n",
    "    rand_num = random()\n",
    "    if rand_num >= 0.3:\n",
    "        return 1\n",
    "    elif 0.1 <= rand_num < 0.3:\n",
    "        return 2\n",
    "    elif 0 <= rand_num < 0.1:\n",
    "        return 3\n",
    "\n",
    "## Append the outcome of the function in a list\n",
    "num_list = []\n",
    "for i in range(100):\n",
    "    x = generate_distribution()\n",
    "    num_list.append(x)\n",
    "\n",
    "\n",
    "## Count the number of each outcome\n",
    "count_dict = {}\n",
    "for i in num_list:\n",
    "    count_dict[i] = count_dict.get(i,0) + 1\n",
    "\n",
    "print('Count of each outcome')\n",
    "pprint.pprint(count_dict)\n",
    "\n",
    "## Probability distribution\n",
    "prob_dict = {}\n",
    "for k,v in count_dict.items():\n",
    "    prob_dict[k] = v/len(num_list)\n",
    "\n",
    "print('Probability of each outcome')\n",
    "pprint.pprint(prob_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of each outcome (as printed above) is close to what the distribution predicts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "2. How are Deep Learning systems different from older Machine Learning systems?\n",
    "\n",
    "**Answer:**\n",
    "- Performance with scale of data: Deep learning algorithms need a large amount of data to understand it perfectly whereas machine learning algorithms with their handcrafted rules prevails with limited data.\n",
    "- Feature Engineering: In Machine learning most of the features need to be identified by an expert and then hand-coded as per teh data type and requirement whereas deep learning algorithms try to learn high-level features from data itself thus  reduces the task of developing new feature extractor for every problem.\n",
    "- Data presentation: Machine learning requires structured data but with deep learning structured data isn’t necessary. In DL the system works with multi-layer neural networks that combine different algorithms and can process unstructured data.\n",
    "- Training: Machine learning requires training phase with human feedback whereas deep learning is self learning system.\n",
    "- Applications: ML is used in simple routine activities whereas DL is used for complex tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "3. There are 4 balls in an urn. Each ball is either red or black. You start by believing that the probabilities that the urn contains 0, 1, 2, 3, 4 red balls are all equal.\n",
    "You then reach into the urn and pull out a ball at random. It is red.\n",
    "Compute the new probabilities that the urn contains 0, 1, 2, 3 or 4 red balls.\n",
    "Hint: Use the Law of Conditional Probabilities\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "4. Answer the following questions:\n",
    "\n",
    "(a) Derive the expression 𝝏𝒚/𝝏𝒙 for the derivative of the Sigmoid Function\n",
    "𝒚 = 𝟏 / 𝟏 + 𝒆𝒙𝒑(−𝒙)\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Derive the expression 𝝏𝒚𝒌/𝝏𝒂𝒊 for the derivative of the Softmax Function (both for 𝒊 = 𝒌 𝒂𝒏𝒅 𝒊 ≠ 𝒌)\n",
    "\n",
    "𝒚𝒌 = 𝒆𝒙𝒑(𝒂𝒌)/Σ 𝒆𝒙𝒑(𝒂𝒊)\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Using the results of Part (b), show that the derivatives 𝝏𝓛/𝝏𝒂𝒌 and 𝝏𝓛/𝝏𝒘𝒌𝒋 for Kary classification problem are given by (see Page 47 of Lecture 3 slides):\n",
    "\n",
    "𝝏𝓛/𝝏𝒂𝒌 = (𝒚𝒌 − 𝒕𝒌) 𝒂𝒏𝒅 𝝏𝓛/𝝏𝒘𝒌𝒋 = 𝒙𝒋 (𝒚𝒌 − 𝒕𝒌)\n",
    "\n",
    "Hint: Use the Chain Rule for multiple variables (Page 6 of Lecture 2)\n",
    "\n",
    "**Answer:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Show that for the case K = 2, the Softmax output Y of the Logistic Regression system, is equivalent to a system in which Y is computed using the Logistic Sigmoid.\n",
    "\n",
    "**Answer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "5. Consider the Linear Model with K = 2, as described on Page 23 of Lecture 3.\n",
    "Suppose that the Cross Entropy Loss Function was replaced by the Mean Square Error (MSE) Loss Function (see Lecture 3, Page 11 for a definition of MSE Loss Function).\n",
    "\n",
    "Compute the gradient 𝝏𝓛/𝝏𝒘𝒊 for the MSE Loss Function.\n",
    "\n",
    "Hint: Follow the steps shown on Lecture 3, Page 31\n",
    "\n",
    "**Answer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
