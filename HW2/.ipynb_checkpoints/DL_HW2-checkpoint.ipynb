{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Assignment 2\n",
    "\n",
    "### Team-  Apurva Shekhar || Dnyanai Surkutwar || Ritu Ranjani Ravi Shankar || Suchita Negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Consider the following computational graph:<img src='imgs/hw2-Q1.jpg'><br>Use the Gradient Flow Rules to compute the gradients for all the arcs in the graph.<br><br>(2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Consider a Deep Feed Forward Network, of the type shown on Page 16 of Lecture 4 Slides,composed of an Input Layer with 10 nodes, followed by a Hidden Layer with 50 nodes and finally an Output Layer with 3 nodes. Assume the activation function f is given by ReLU, and the output function h is given by Softmax.<br><ul><li>a.  Compute the number of parameters (weights and biases) required to describe the network.</li><br><li>b. Write a Python program to do a forward pass through the network. Assume that the input training sample X = (0.5, 0.6, 0.1, 0.25, 0.33, 0.9, 0.88, 0.76, 0.69, 0.95). Also assume that all the weights and biases are initialized according to a Uniform distribution between [0,0.1]</li><br><li>c. Assuming that the output Label corresponding to X, is T = (1, 0, 0). Using the results of part (b), compute the Cross Entropy Loss Function.In order to verify the forward pass, the Loss Function should compute to log(3),explain why.</li><br><li>d. Write a Python program to do a backward pass through the network, and compute all the delta values.</li><br><li>e. Finally extend the Python program to compute the new weight parameters using stochastic gradient descent.<br>In order to verify that the algorithm has been coded correctly, you can do the following: Put the forward and backward passes in a loop (with the single training sample given in Part (b) and the corresponding Label in Part (c)). After several iterations you should see the Loss Function go to zero (explain why).</ul><br><br>(15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) <img src=\"imgs/Q2a.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The input training vector X is:\n",
      "  [0.5  0.6  0.1  0.25 0.33 0.9  0.88 0.76 0.69 0.95]\n",
      "\n",
      "\n",
      "The initial weights vector from input layer to hidden layer is:\n",
      "  [0.06542492 0.01618296 0.03078716 0.05930982 0.06082688 0.02309642\n",
      " 0.06348129 0.02458417 0.00282151 0.08438698]\n",
      "\n",
      "\n",
      "The initial weights vector form hidden layer to output layer is:\n",
      "  [0.09247599 0.06675714 0.09644582 0.06330467 0.00238061 0.00421894\n",
      " 0.06203102 0.08404259 0.02011886 0.00819451]\n",
      "\n",
      "\n",
      "The initial bias vector for the hidden layer is:\n",
      "  [0.0086479  0.08680867 0.02008624 0.09161796 0.06396594 0.0726715\n",
      " 0.05856926 0.03584538 0.01083557 0.04247873]\n",
      "\n",
      "\n",
      "The initial bias vector for the output layer is:\n",
      "  [0.0465026  0.09294556 0.03326734 0.09870198 0.07942789 0.01220153\n",
      " 0.03452445 0.02656599 0.05716257 0.01555546]\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "z=f(x) given by:  [0.26649792 0.34465869 0.27793626 0.34946798 0.32181596 0.33052152\n",
      " 0.31641928 0.2936954  0.26868559 0.30032875]\n",
      "\n",
      "\n",
      "y=f(z) given by:  [0.0956038  0.10829038 0.09543216 0.10944061 0.10442362 0.09848804\n",
      " 0.09930101 0.09630049 0.09684001 0.09587989]\n"
     ]
    }
   ],
   "source": [
    "## importing required libaries\n",
    "import numpy as np\n",
    "\n",
    "class NNsetup:\n",
    "    \n",
    "    def __init__(self,X,T):\n",
    "        self.x = X\n",
    "        self.wts_i2h = np.array(np.random.uniform(0,0.1,len(self.x)))\n",
    "        self.wts_h2o = np.array(np.random.uniform(0,0.1,len(self.x)))\n",
    "        self.b_h = np.array(np.random.uniform(0,0.1,len(self.x)))\n",
    "        self.b_o = np.array(np.random.uniform(0,0.1,len(self.x)))\n",
    "        self.t = T         \n",
    "        \n",
    "    def activation_ReLU(self,layer1):\n",
    "        return np.maximum(0,layer1)\n",
    "    \n",
    "    def output_Softmax(self,layer1):\n",
    "        numerator = np.exp(layer1 + self.b_o)\n",
    "        #print('num:',numerator)\n",
    "        denominator = np.sum(np.exp(layer1 + self.b_o))\n",
    "        #print('denum:',denominator)\n",
    "        return numerator/denominator\n",
    "        \n",
    "           \n",
    "    def pre_activation(self):\n",
    "        ## Input to hidden layer operation = nodewise calculation like a1(1) = wT.x + b\n",
    "        return np.dot(self.wts_i2h,self.x) + self.b_h\n",
    "    \n",
    "## Training Input: \n",
    "X = np.array([0.5, 0.6, 0.1, 0.25, 0.33, 0.9, 0.88, 0.76, 0.69, 0.95])\n",
    "T = np.array([1, 0, 0])\n",
    "\n",
    "X_train = NNsetup(X,T)\n",
    "\n",
    "## The input set up:\n",
    "print('\\n\\nThe input training vector X is:\\n ',X_train.x)\n",
    "print('\\n\\nThe initial weights vector from input layer to hidden layer is:\\n ',X_train.wts_i2h)\n",
    "print('\\n\\nThe initial weights vector form hidden layer to output layer is:\\n ',X_train.wts_h2o)\n",
    "print('\\n\\nThe initial bias vector for the hidden layer is:\\n ',X_train.b_h)\n",
    "print('\\n\\nThe initial bias vector for the output layer is:\\n ',X_train.b_o)\n",
    "\n",
    "print('\\n'+'-'*120)\n",
    "\n",
    "## Forward pass:\n",
    "#Step1: a = wx+b\n",
    "a = X_train.pre_activation()\n",
    "\n",
    "#Step2: z = f(a)\n",
    "z = X_train.activation_ReLU(a)\n",
    "print('\\n\\nz=f(x) given by: ',z)\n",
    "\n",
    "#Step3: o = f(z)\n",
    "o = X_train.output_Softmax(z)\n",
    "print('\\n\\ny=f(z) given by: ',o)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The MNIST_Fashion dataset comes with Keras, and can be loaded using the commands: fashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() This dataset corresponds to grayscale coded images of various clothing items. Just as in MNIST, there are 10 classes and each image is a 28*28 matrix (see https://www.tensorflow.org/tutorials/keras/basic_classification for more information about this dataset). Answer the following:<br><ul><li>a. What is the best classification accuracy that you can achieve with the Linear Model on this dataset?</li><li>b. Replace the Linear model with a Fully Connected Feed Forward Model with one or more Hidden Layers. What is the best classification accuracy that you can achieve with this model? In order to get better performance , you can vary the number of hidden layers and the number of nodes per hidden layer</li></ul><br><br>(8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The figure above shows an example of Ensemble Processing (see Lecture 4). Start with the best model that you have after doing Problem 3, and replicate that model 3 times in a parallel configuration as shown in the figure (use the Keras Functional API for doing this). Take the final logit nodes from each branch and average it to create the final logit layer for the model (see https://keras.io/api/layers/merging_layers/average/ on how to do this). Train the model again on the fashion_mnist dataset, and check for an improvement in prediction accuracy (if any).<img src=\"imgs/hw2-Q4.jpg\"><br><br>(5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
