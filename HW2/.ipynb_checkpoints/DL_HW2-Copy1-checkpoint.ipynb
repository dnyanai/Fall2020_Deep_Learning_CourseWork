{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Assignment 2\n",
    "\n",
    "### Team-  Apurva Shekhar || Dnyanai Surkutwar || Ritu Ranjani Ravi Shankar || Suchita Negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Consider the following computational graph:<img src='imgs/hw2-Q1.jpg'><br>Use the Gradient Flow Rules to compute the gradients for all the arcs in the graph.<br><br>(2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Consider a Deep Feed Forward Network, of the type shown on Page 16 of Lecture 4 Slides,composed of an Input Layer with 10 nodes, followed by a Hidden Layer with 50 nodes and finally an Output Layer with 3 nodes. Assume the activation function f is given by ReLU, and the output function h is given by Softmax.<br><ul><li>a.  Compute the number of parameters (weights and biases) required to describe the network.</li><br><li>b. Write a Python program to do a forward pass through the network. Assume that the input training sample X = (0.5, 0.6, 0.1, 0.25, 0.33, 0.9, 0.88, 0.76, 0.69, 0.95). Also assume that all the weights and biases are initialized according to a Uniform distribution between [0,0.1]</li><br><li>c. Assuming that the output Label corresponding to X, is T = (1, 0, 0). Using the results of part (b), compute the Cross Entropy Loss Function.In order to verify the forward pass, the Loss Function should compute to log(3),explain why.</li><br><li>d. Write a Python program to do a backward pass through the network, and compute all the delta values.</li><br><li>e. Finally extend the Python program to compute the new weight parameters using stochastic gradient descent.<br>In order to verify that the algorithm has been coded correctly, you can do the following: Put the forward and backward passes in a loop (with the single training sample given in Part (b) and the corresponding Label in Part (c)). After several iterations you should see the Loss Function go to zero (explain why).</ul><br><br>(15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) <img src=\"imgs/hw2-Q2a.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Neural Network - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class model_setup:\n",
    "    \n",
    "    def __init__(self,X,T):\n",
    "        \n",
    "        self.X = X\n",
    "        self.T = T ## T labels for Q2) c) \n",
    "        \n",
    "    def model_layer_sizes(self):\n",
    "        n_x = self.X.shape[0] ## 10 rows, 1 column \n",
    "        n_h = 50    \n",
    "        n_y = self.T.shape[0] ## 3\n",
    "        return (n_x,n_h,n_y)\n",
    "\n",
    "    def parameters_initialize(self):\n",
    "        np.random.seed(50)\n",
    "        ## Wts and biases for the first layer i.e. hidden layer,\n",
    "        self.W1 = np.random.uniform(0,0.1,(n_h,n_x))\n",
    "        self.b1 = np.full((n_h,1),np.random.uniform(0,0.1))\n",
    "\n",
    "        ## Wts and biases for the output layer,\n",
    "        self.W2 = np.random.uniform(0,0.1,(n_y,n_h))\n",
    "        self.b2 = np.full((n_y,1),(np.random.uniform(0,0.1)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        params = {\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "        return params\n",
    "    \n",
    "    def pre_activation(self,layer,w,b): ## Step 1\n",
    "        return (np.dot(w,layer)+b)\n",
    "        \n",
    "    def activation_ReLU(self,z):       ## Step 2\n",
    "        return np.maximum(0,z)\n",
    "        \n",
    "    def output_Softmax(self,z2):      ## Step 3\n",
    "        numerator = np.exp(z2)\n",
    "        denominator = sum(np.exp(z2))\n",
    "        return numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X_train):\n",
    "    ## Step1: Setting up the model layers and their vector shapes:\n",
    "    (n_x,n_h,n_y) = X_train.model_layer_sizes()\n",
    "\n",
    "    ## Step2: Creating parameters in accordance to the training samples for the respective layers of the model \n",
    "#    params = X_train.parameters_initialize()\n",
    "    \n",
    "    ## Step3: Caculate the forward pass: A1,Z1\n",
    "    Z1 = X_train.pre_activation(X_train.X,params[\"W1\"],params[\"b1\"])    ## 1st layer - [wT].[x] + [b1] \n",
    "   \n",
    "    A1 = X_train.activation_ReLU(Z1)                        ## Activation fn - ReLU\n",
    "    \n",
    "    Z2 = X_train.pre_activation(A1,params[\"W2\"],params[\"b2\"])              ## 2nd layer - [wT].[x] + [b2]\n",
    "    \n",
    "    Y = X_train.output_Softmax(Z2) ## A2                   ## Output fn - Softmax\n",
    "   \n",
    "    return {\"A1\": A1, \"Z1\": Z1, \"Y\": Y, \"Z2\": Z2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8d02e71a351f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_setup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.33\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.88\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.76\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.69\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfwd_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## Answer b)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The size of the input layer is: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1ffac46e04d3>\u001b[0m in \u001b[0;36mforward_pass\u001b[1;34m(X_train)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m## Step2: Creating parameters in accordance to the training samples for the respective layers of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m## Step3: Caculate the forward pass: A1,Z1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-610f699b2a42>\u001b[0m in \u001b[0;36mparameters_initialize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m## Wts and biases for the first layer i.e. hidden layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mW1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_h' is not defined"
     ]
    }
   ],
   "source": [
    "##Main function:\n",
    "X_train = model_setup(np.array([[0.5],[0.6],[0.1],[0.25],[0.33],[0.9],[0.88],[0.76],[0.69],[0.95]]),np.array([[1],[0],[0]]))\n",
    "\n",
    "fwd_result = forward_pass(X_train) ## Answer b) \n",
    "\n",
    "print('The size of the input layer is: ',n_x)\n",
    "print('The size of the hidden layer is: ',n_h)\n",
    "print('The size of the output layer is: ',n_y)\n",
    "print('-'*100)\n",
    "\n",
    "\n",
    "print('The size of the weights for the 1st layer i.e. hidden layer is:',params[\"W1\"].shape)\n",
    "print('The size of the bias for the 1st layer i.e. hidden layer is:',params[\"b1\"].shape)\n",
    "print('The size of the weights for the 2st layer i.e. output layer is:',params[\"W2\"].shape)\n",
    "print('The size of the bias for the 2st layer i.e. output layer is:',params[\"b2\"].shape)\n",
    "print('-'*100)\n",
    "\n",
    "print('The size of the pre-activation calculation is :',Z1.shape)\n",
    "print('-'*100)\n",
    "\n",
    "print('The size of the activation matrix is :',A1.shape)\n",
    "print('-'*100)\n",
    "\n",
    "print('The size of the 2nd layer pre-activation is :',Z2.shape)\n",
    "print('-'*100)\n",
    "\n",
    "print('The size of the output layer is :',Y.shape)\n",
    "print('-'*100)\n",
    "\n",
    "print('The output vector is :\\n',Y)\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Cross Entropy, $- \\sum\\limits_{i=0}^{k}  t^{(i)}\\log(Y{(i)})$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(Y,Tlabel):\n",
    "    ## Above T is referring to the Ground Truth and Y is referring to the forward pass output vector - \n",
    "    return (-sum(np.dot(np.log(Y).T,Tlabel))) ## (3,1) . (3,1) so k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = calculate_loss(Y,X_train.T)\n",
    "print('The Cross Entropy Loss is:\\n ',loss[0])\n",
    "print('\\nTo verify the forward pass we will take log(3): \\n',np.log(3))\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X_train, params, fwd_result):\n",
    "    m = X_train.X.shape[1]\n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = params[\"W1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and Y from dictionary \"cache\".\n",
    "    A1 = fwd_result[\"A1\"]\n",
    "    Y = fwd_result[\"Y\"]\n",
    "  \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = Y - X_train.T\n",
    "    dW2 = np.dot(dZ2,np.transpose(A1))/m\n",
    "    db2 = np.sum(dZ2,axis = 1, keepdims = True)/m\n",
    "    dZ1 = np.dot(np.transpose(W2),dZ2) * (1-np.power(A1,2))\n",
    "    dW1 = np.dot(dZ1,np.transpose(X_train.X)) /m\n",
    "    db1 = np.sum(dZ1, axis =1, keepdims = True)/m\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads =  backward_pass(X_train, params, fwd_result)\n",
    "grads_shape = {\"dW1\": grads[\"dW1\"].shape, \n",
    "               \"db1\": grads[\"db1\"].shape, \n",
    "               \"dW2\": grads[\"dW2\"].shape, \n",
    "               \"db2\": grads[\"db2\"].shape}\n",
    "print('All delta matrices shapes are', grads_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate = 1.2):\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2'] \n",
    "    # Update rule for each parameter for each data points of the entire dataset.\n",
    "    W1 = W1 - (learning_rate * dW1)\n",
    "    b1 = b1 - (learning_rate * db1) \n",
    "    W2 = W2 - (learning_rate * dW2)\n",
    "    b2 = b2 - (learning_rate * db2)\n",
    "    # Return updated params.\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, X_train.X.shape[0]):\n",
    "            X = X_train.X[i]\n",
    "            model_SGD = X_train\n",
    "            \n",
    "            # Forward propagation.\n",
    "            fwd_result = forward_pass(model_SGD)\n",
    "            \n",
    "            \n",
    "            # Backward propogation\n",
    "            grads = backward_pass(X_train, params, fwd_result)\n",
    "            \n",
    "            # Update weights\n",
    "            params = update_params(params, grads)\n",
    "            print(\"Updated param W1 for input\",i,\"is\")\n",
    "            print(params[\"W1\"][:1])  ## Showing only first 1 weights to show comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(dataset, T , params,num_iterations = 100):\n",
    "    for k in range(0, num_iterations):\n",
    "        loss = 0\n",
    "        for i in range(0, X_train.X.shape[0]):\n",
    "            X = dataset[i]\n",
    "            model = X_train\n",
    "            # Forward propagation.\n",
    "            fwd_result = forward_pass(model, params)\n",
    "            # Calculate loss\n",
    "            loss += calculate_loss(fwd_result[\"Y\"], T)\n",
    "            # Backward propogation\n",
    "            grads = backward_pass(model, params, fwd_result)\n",
    "            # Update weights\n",
    "            params = update_params(params, grads)\n",
    "        # Print the cost every 100 iterations\n",
    "        if i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (k, loss))\n",
    "\n",
    "#dataset = [X]\n",
    "neural_network(X_train.X, X_train.T, copy.deepcopy(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The MNIST_Fashion dataset comes with Keras, and can be loaded using the commands: fashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() This dataset corresponds to grayscale coded images of various clothing items. Just as in MNIST, there are 10 classes and each image is a 28*28 matrix (see https://www.tensorflow.org/tutorials/keras/basic_classification for more information about this dataset). Answer the following:<br><ul><li>a. What is the best classification accuracy that you can achieve with the Linear Model on this dataset?</li><li>b. Replace the Linear model with a Fully Connected Feed Forward Model with one or more Hidden Layers. What is the best classification accuracy that you can achieve with this model? In order to get better performance , you can vary the number of hidden layers and the number of nodes per hidden layer</li></ul><br><br>(8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The figure above shows an example of Ensemble Processing (see Lecture 4). Start with the best model that you have after doing Problem 3, and replicate that model 3 times in a parallel configuration as shown in the figure (use the Keras Functional API for doing this). Take the final logit nodes from each branch and average it to create the final logit layer for the model (see https://keras.io/api/layers/merging_layers/average/ on how to do this). Train the model again on the fashion_mnist dataset, and check for an improvement in prediction accuracy (if any).<img src=\"imgs/hw2-Q4.jpg\"><br><br>(5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
