{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the weights for the 1st layer i.e. hidden layer is: (50, 10)\n",
      "The size of the bias for the 1st layer i.e. hidden layer is: (50, 1)\n",
      "The size of the weights for the 2st layer i.e. output layer is: (3, 50)\n",
      "The size of the bias for the 2st layer i.e. output layer is: (3, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the input layer is:  10\n",
      "The size of the hidden layer is:  50\n",
      "The size of the output layer is:  3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the pre-activation calculation is : (50, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the activation matrix is : (50, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the 2nd layer pre-activation is : (3, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the output layer is : (3, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The output vector is :\n",
      " [[0.34179754]\n",
      " [0.31943096]\n",
      " [0.3387715 ]]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class Model:\n",
    "    def __init__(self,X,T):\n",
    "        self.X = X\n",
    "        self.T = T ## T labels\n",
    "        \n",
    "    def layer_sizes(self):\n",
    "        n_x = self.X.shape[0] ## 10 rows, 1 column \n",
    "        n_h = 50    \n",
    "        n_y = self.T.shape[0] ## 3\n",
    "        return (n_x,n_h,n_y)\n",
    "\n",
    "    def pre_activation(self,layer,w,b): ## Step 1\n",
    "        return (np.dot(w,layer)+b)\n",
    "        \n",
    "    def activation_ReLU(self,p):       ## Step 2\n",
    "        return np.maximum(0,p)\n",
    "        \n",
    "    def output_Softmax(self,z2):      ## Step 3\n",
    "        numerator = np.exp(z2)\n",
    "        denominator = sum(np.exp(z2))\n",
    "        return numerator/denominator\n",
    "\n",
    "def generate_params(model):\n",
    "    (n_x, n_h, n_y) = model.layer_sizes()\n",
    "    np.random.seed(50)\n",
    "    ## Wts and biases for the first layer i.e. hidden layer,\n",
    "    W1 = np.random.uniform(0,0.1,(n_h,n_x))\n",
    "    b1 = np.full((n_h,1),np.random.uniform(0,0.1))\n",
    "    ## Wts and biases for the output layer,\n",
    "    W2 = np.random.uniform(0,0.1,(n_y,n_h))\n",
    "    b2 = np.full((n_y,1),(np.random.uniform(0,0.1)))\n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    return {\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "    \n",
    "def forward_pass(X_train, params):\n",
    "    (n_x,n_h,n_y) = X_train.layer_sizes()\n",
    "    print('The size of the input layer is: ',n_x)\n",
    "    print('The size of the hidden layer is: ',n_h)\n",
    "    print('The size of the output layer is: ',n_y)\n",
    "    print('-'*100)\n",
    "    ## Step3: Caculate the forward pass: A1,Z1\n",
    "    # Pre-activation:\n",
    "    Z1 = X_train.pre_activation(X_train.X,params[\"W1\"],params[\"b1\"])    ## 1st layer - [wT].[x] + [b1] \n",
    "    print('The size of the pre-activation calculation is :',Z1.shape)\n",
    "    print('-'*100)\n",
    "\n",
    "    A1 = X_train.activation_ReLU(Z1)                        ## Activation fn - ReLU\n",
    "    print('The size of the activation matrix is :',A1.shape)\n",
    "    print('-'*100)\n",
    "\n",
    "    Z2 = X_train.pre_activation(A1,params[\"W2\"],params[\"b2\"])              ## 2nd layer - [wT].[x] + [b2]\n",
    "    print('The size of the 2nd layer pre-activation is :',Z2.shape)\n",
    "    print('-'*100)\n",
    "\n",
    "    Y = X_train.output_Softmax(Z2) ## A2                   ## Output fn - Softmax\n",
    "    print('The size of the output layer is :',Y.shape)\n",
    "    print('-'*100)\n",
    "\n",
    "    print('The output vector is :\\n',Y)\n",
    "    print('-'*100)\n",
    "    return {\"A1\": A1, \"Z1\": Z1, \"Y\": Y, \"Z2\": Z2}\n",
    "\n",
    "##Main function:\n",
    "X = np.array([[0.5],[0.6],[0.1],[0.25],[0.33],[0.9],[0.88],[0.76],[0.69],[0.95]])\n",
    "T = np.array([[1],[0],[0]])\n",
    "## Step1: Setting up the model layers and their vector shapes:\n",
    "model = Model(X, T)\n",
    "## Step 2: Initialize parameters.\n",
    "params = generate_params(model)\n",
    "print('The size of the weights for the 1st layer i.e. hidden layer is:',params[\"W1\"].shape)\n",
    "print('The size of the bias for the 1st layer i.e. hidden layer is:',params[\"b1\"].shape)\n",
    "print('The size of the weights for the 2st layer i.e. output layer is:',params[\"W2\"].shape)\n",
    "print('The size of the bias for the 2st layer i.e. output layer is:',params[\"b2\"].shape)\n",
    "print('-'*100)\n",
    "fwd_result = forward_pass(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cross Entropy Loss is:\n",
      "  [1.07353671]\n",
      "\n",
      "To verify the forward pass we will take log(3): \n",
      " 1.0986122886681098\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.07353671])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Above T is referring to the Ground Truth and Y is referring to the forward pass output vector - \n",
    "# cross_entropy_mul = -sum(np.multiply(np.log10(Y),X_train.T))\n",
    "#print(Y.shape)\n",
    "#print(X_train.T.shape)\n",
    "#print(Y)\n",
    "#print(X_train.T)\n",
    "def calculate_loss(Y, T):\n",
    "    cross_entropy = -sum(np.dot(np.log(Y).T, model.T)) ## (3,1) . (3,1) so k = 3\n",
    "    print('The Cross Entropy Loss is:\\n ',cross_entropy)\n",
    "    print('\\nTo verify the forward pass we will take log(3): \\n',np.log(3))\n",
    "    print('-'*100)\n",
    "    return cross_entropy\n",
    "calculate_loss(fwd_result[\"Y\"], model.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X_train, params, fwd_result):\n",
    "    m = X_train.X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = params[\"W1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and Y from dictionary \"cache\".\n",
    "    A1 = fwd_result[\"A1\"]\n",
    "    Y = fwd_result[\"Y\"]\n",
    "  \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = Y - X_train.T\n",
    "    dW2 = np.dot(dZ2,np.transpose(A1))/m\n",
    "    db2 = np.sum(dZ2,axis = 1, keepdims = True)/m\n",
    "    dZ1 = np.dot(np.transpose(W2),dZ2) * (1-np.power(A1,2))\n",
    "    dW1 = np.dot(dZ1,np.transpose(X_train.X)) /m\n",
    "    db1 = np.sum(dZ1, axis =1, keepdims = True)/m\n",
    "\n",
    "    \n",
    "    delta_dict = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    delta_dict_shape = {\"dW1\": delta_dict[\"dW1\"].shape, \n",
    "                        \"db1\": delta_dict[\"db1\"].shape, \n",
    "                        \"dW2\": delta_dict[\"dW2\"].shape, \n",
    "                        \"db2\": delta_dict[\"db2\"].shape}\n",
    "    print('All delta matrices shapes are', delta_dict_shape)\n",
    "    \n",
    "    return delta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All delta matrices shapes are {'dW1': (50, 10), 'db1': (50, 1), 'dW2': (3, 50), 'db2': (3, 1)}\n"
     ]
    }
   ],
   "source": [
    "grads =  backward_pass(model, params, fwd_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate = 1.2):\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2'] \n",
    "    # Update rule for each parameter for each data points of the entire dataset.\n",
    "    W1 = W1 - (learning_rate * dW1)\n",
    "    b1 = b1 - (learning_rate * db1) \n",
    "    W2 = W2 - (learning_rate * dW2)\n",
    "    b2 = b2 - (learning_rate * db2)\n",
    "    # Return updated params.\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the input layer is:  1\n",
      "The size of the hidden layer is:  50\n",
      "The size of the output layer is:  3\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (50,10) and (1,1) not aligned: 10 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0544da1b0b07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Cost after iteration %i: %f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mneural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-0544da1b0b07>\u001b[0m in \u001b[0;36mneural_network\u001b[0;34m(model, params, num_iterations)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Forward propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mfwd_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwd_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a341f1143f6e>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(X_train, params)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m## Step3: Caculate the forward pass: A1,Z1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Pre-activation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m## 1st layer - [wT].[x] + [b1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The size of the pre-activation calculation is :'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a341f1143f6e>\u001b[0m in \u001b[0;36mpre_activation\u001b[0;34m(self, layer, w, b)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpre_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m## Step 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mactivation_ReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0;31m## Step 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (50,10) and (1,1) not aligned: 10 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "def neural_network(model, params, num_iterations = 10000):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    for k in range(0, num_iterations):\n",
    "        loss = 0\n",
    "        for i in range(0, model.X.shape[0]):\n",
    "            xi = model.X[i:i+1, :]\n",
    "            # yi = model.T[i:i+1, :]\n",
    "            new_model = Model(xi, model.T)\n",
    "            \n",
    "            # Forward propagation.\n",
    "            fwd_result = forward_pass(new_model, params)\n",
    "            loss += calculate_loss(Y, new_model.T)\n",
    "            grads = backward_pass(new_model, params, fwd_result)\n",
    "            params = update_params(params, grads)\n",
    "        # Print the cost every 1000 iterations\n",
    "        if i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (k, loss))\n",
    "\n",
    "neural_network(model, copy.deepcopy(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
