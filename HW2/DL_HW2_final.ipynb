{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL Assignment 2\n",
    "\n",
    "### Team-  Apurva Shekhar || Dnyanai Surkutwar || Ritu Ranjani Ravi Shankar || Suchita Negi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Consider the following computational graph:<img src='imgs/hw2-Q1.jpg'><br>Use the Gradient Flow Rules to compute the gradients for all the arcs in the graph.<br><br>(2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Consider a Deep Feed Forward Network, of the type shown on Page 16 of Lecture 4 Slides,composed of an Input Layer with 10 nodes, followed by a Hidden Layer with 50 nodes and finally an Output Layer with 3 nodes. Assume the activation function f is given by ReLU, and the output function h is given by Softmax.<br><ul><li>a.  Compute the number of parameters (weights and biases) required to describe the network.</li><br><li>b. Write a Python program to do a forward pass through the network. Assume that the input training sample X = (0.5, 0.6, 0.1, 0.25, 0.33, 0.9, 0.88, 0.76, 0.69, 0.95). Also assume that all the weights and biases are initialized according to a Uniform distribution between [0,0.1]</li><br><li>c. Assuming that the output Label corresponding to X, is T = (1, 0, 0). Using the results of part (b), compute the Cross Entropy Loss Function.In order to verify the forward pass, the Loss Function should compute to log(3),explain why.</li><br><li>d. Write a Python program to do a backward pass through the network, and compute all the delta values.</li><br><li>e. Finally extend the Python program to compute the new weight parameters using stochastic gradient descent.<br>In order to verify that the algorithm has been coded correctly, you can do the following: Put the forward and backward passes in a loop (with the single training sample given in Part (b) and the corresponding Label in Part (c)). After several iterations you should see the Loss Function go to zero (explain why).</ul><br><br>(15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) <img src=\"imgs/hw2-Q2a.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Neural Network - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class model_setup:\n",
    "    \n",
    "    def __init__(self,X,T):\n",
    "        \n",
    "        self.X = X\n",
    "        self.T = T ## T labels for Q2) c) \n",
    "        \n",
    "    def layer_sizes(self):\n",
    "        n_x = self.X.shape[0] ## 10 rows, 1 column \n",
    "        n_h = 50    \n",
    "        n_y = self.T.shape[0] ## 3\n",
    "        return (n_x,n_h,n_y)\n",
    "        \n",
    "    def generate_params(self,model):\n",
    "        (n_x, n_h, n_y) = model.layer_sizes()\n",
    "        np.random.seed(50)\n",
    "        ## Wts and biases for the first layer i.e. hidden layer\n",
    "        W1 = np.random.uniform(0,0.1,(n_h,n_x))\n",
    "        b1 = np.full((n_h,1),np.random.uniform(0,0.1))\n",
    "        ## Wts and biases for the output layer\n",
    "        W2 = np.random.uniform(0,0.1,(n_y,n_h))\n",
    "        b2 = np.full((n_y,1),(np.random.uniform(0,0.1)))\n",
    "        \n",
    "        assert (W1.shape == (n_h, n_x))\n",
    "        assert (b1.shape == (n_h, 1))\n",
    "        assert (W2.shape == (n_y, n_h))\n",
    "        assert (b2.shape == (n_y, 1))\n",
    "        \n",
    "        return {\"W1\":W1,\"b1\":b1,\"W2\":W2,\"b2\":b2}\n",
    "\n",
    "    def pre_activation(self,layer,w,b): ## Step 1\n",
    "        return (np.dot(w,layer)+b)\n",
    "        \n",
    "    def activation_ReLU(self,z):       ## Step 2\n",
    "        return np.maximum(0,z)\n",
    "        \n",
    "    def output_Softmax(self,z2):      ## Step 3\n",
    "        numerator = np.exp(z2)\n",
    "        denominator = sum(np.exp(z2))\n",
    "        return numerator/denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(train_model,params):\n",
    "    ## Step1: Setting up the model layers and their vector shapes:\n",
    "    (n_x,n_h,n_y) = train_model.layer_sizes()\n",
    "\n",
    "    ##Caculate the forward pass: A1,Z1\n",
    "    Z1 = train_model.pre_activation(train_model.X,params[\"W1\"],params[\"b1\"])    ## 1st layer - [wT].[x] + [b1] \n",
    "   \n",
    "    A1 = train_model.activation_ReLU(Z1)                        ## Activation fn - ReLU\n",
    "    \n",
    "    Z2 = train_model.pre_activation(A1,params[\"W2\"],params[\"b2\"])              ## 2nd layer - [wT].[x] + [b2]\n",
    "    \n",
    "    Y = train_model.output_Softmax(Z2) ## A2                   ## Output fn - Softmax\n",
    "   \n",
    "    return {\"A1\": A1, \"Z1\": Z1, \"Y\": Y, \"Z2\": Z2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Main function:\n",
    "train_model = model_setup(np.array([[0.5],[0.6],[0.1],[0.25],[0.33],[0.9],[0.88],[0.76],[0.69],[0.95]]),np.array([[1],[0],[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the model: (input=10, hidden=50, output=3)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the weights for the 1st layer i.e. hidden layer is: (50, 10)\n",
      "The size of the bias for the 1st layer i.e. hidden layer is: (50, 1)\n",
      "The size of the weights for the 2st layer i.e. output layer is: (3, 50)\n",
      "The size of the bias for the 2st layer i.e. output layer is: (3, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the pre-activation calculation is : (50, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the activation matrix is : (50, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the 2nd layer pre-activation is : (3, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The size of the output layer is : (3, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "The output vector is :\n",
      " [[0.34179754]\n",
      " [0.31943096]\n",
      " [0.3387715 ]]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## Step1: Setting up the model layers and their vector shapes:\n",
    "print('The size of the model: (input=%i, hidden=%i, output=%i)' % train_model.layer_sizes())\n",
    "print('-'*100)\n",
    "\n",
    "## Step 2: Initialize parameters.\n",
    "params = train_model.generate_params(train_model)\n",
    "print('The size of the weights for the 1st layer i.e. hidden layer is:',params[\"W1\"].shape)\n",
    "print('The size of the bias for the 1st layer i.e. hidden layer is:',params[\"b1\"].shape)\n",
    "print('The size of the weights for the 2st layer i.e. output layer is:',params[\"W2\"].shape)\n",
    "print('The size of the bias for the 2st layer i.e. output layer is:',params[\"b2\"].shape)\n",
    "print('-'*100)\n",
    "\n",
    "## Forward pass\n",
    "fwd_result = forward_pass(train_model,params)\n",
    "print('The size of the pre-activation calculation is :', fwd_result[\"Z1\"].shape)\n",
    "print('-'*100)\n",
    "print('The size of the activation matrix is :', fwd_result[\"A1\"].shape)\n",
    "print('-'*100)\n",
    "print('The size of the 2nd layer pre-activation is :', fwd_result[\"Z2\"].shape)\n",
    "print('-'*100)\n",
    "print('The size of the output layer is :', fwd_result[\"Y\"].shape)\n",
    "print('-'*100)\n",
    "print('The output vector is :\\n', fwd_result[\"Y\"])\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Cross Entropy, $- \\sum\\limits_{i=0}^{k}  t^{(i)}\\log(Y{(i)})$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(Y,Tlabel):\n",
    "    ## Above T is referring to the Ground Truth and Y is referring to the forward pass output vector - \n",
    "    return (-sum(np.dot(np.log(Y).T,Tlabel))) ## (3,1) . (3,1) so k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cross Entropy Loss is:\n",
      "  1.073536706303381\n",
      "\n",
      "To verify the forward pass we will take log(3): \n",
      " 1.0986122886681098\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "loss = calculate_loss(fwd_result[\"Y\"],train_model.T)\n",
    "print('The Cross Entropy Loss is:\\n ',loss[0])\n",
    "print('\\nTo verify the forward pass we will take log(3): \\n',np.log(3))\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaination:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(train_model, params, fwd_result):\n",
    "    m = train_model.X.shape[1]\n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    W1 = params[\"W1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "        \n",
    "    # Retrieve also A1 and Y from dictionary \"cache\".\n",
    "    A1 = fwd_result[\"A1\"]\n",
    "    Y = fwd_result[\"Y\"]\n",
    "  \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = Y - train_model.T\n",
    "    dW2 = np.dot(dZ2,np.transpose(A1))/m\n",
    "    db2 = np.sum(dZ2,axis = 1, keepdims = True)/m\n",
    "    dZ1 = np.dot(np.transpose(W2),dZ2) * (1-np.power(A1,2))\n",
    "    dW1 = np.dot(dZ1,np.transpose(train_model.X)) /m\n",
    "    db1 = np.sum(dZ1, axis =1, keepdims = True)/m\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All delta matrices shapes are {'dW1': (50, 10), 'db1': (50, 1), 'dW2': (3, 50), 'db2': (3, 1)}\n"
     ]
    }
   ],
   "source": [
    "grads =  backward_pass(train_model, params, fwd_result)\n",
    "grads_shape = {\"dW1\": grads[\"dW1\"].shape, \n",
    "               \"db1\": grads[\"db1\"].shape, \n",
    "               \"dW2\": grads[\"dW2\"].shape, \n",
    "               \"db2\": grads[\"db2\"].shape}\n",
    "print('All delta matrices shapes are', grads_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, learning_rate = 1.2):\n",
    "    W1 = params[\"W1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"W2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads['dW1']\n",
    "    db1 = grads['db1']\n",
    "    dW2 = grads['dW2']\n",
    "    db2 = grads['db2'] \n",
    "    # Update rule for each parameter for each data points of the entire dataset.\n",
    "    W1 = W1 - (learning_rate * dW1)\n",
    "    b1 = b1 - (learning_rate * db1) \n",
    "    W2 = W2 - (learning_rate * dW2)\n",
    "    b2 = b2 - (learning_rate * db2)\n",
    "    # Return updated params.\n",
    "    return {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated param W1 for input 0 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 1 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 2 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 3 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 4 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 5 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 6 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 7 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 8 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n",
      "Updated param W1 for input 9 is\n",
      "[[0.06855731 0.04572488 0.02936682 0.04918156 0.05033562 0.13403228\n",
      "  0.07443069 0.10621706 0.10240773 0.06728551]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, train_model.X.shape[0]):\n",
    "            params_SGD = params.copy()\n",
    "            X = train_model.X[i]\n",
    "            model_SGD = train_model\n",
    "            \n",
    "            # Forward propagation.\n",
    "            fwd_result = forward_pass(model_SGD,params_SGD)\n",
    "            \n",
    "            # loss:\n",
    "            loss = calculate_loss(fwd_result[\"Y\"], train_model.T)\n",
    "\n",
    "            # Backward propogation\n",
    "            \n",
    "            grads_SGD = grads\n",
    "            grads = backward_pass(train_model, params_SGD, fwd_result)\n",
    "            \n",
    "            # Update weights\n",
    "            params_SGD = update_params(params_SGD,grads_SGD)\n",
    "            print(\"Updated param W1 for input\",i,\"is\")\n",
    "            print(params_SGD[\"W1\"][:1])  ## Showing only first 1 weights to show comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.073537\n",
      "Cost after iteration 1: 0.000081\n",
      "Cost after iteration 2: 0.000080\n",
      "Cost after iteration 3: 0.000080\n",
      "Cost after iteration 4: 0.000079\n",
      "Cost after iteration 5: 0.000079\n",
      "Cost after iteration 6: 0.000079\n",
      "Cost after iteration 7: 0.000078\n",
      "Cost after iteration 8: 0.000078\n",
      "Cost after iteration 9: 0.000077\n",
      "Cost after iteration 10: 0.000077\n",
      "Cost after iteration 11: 0.000076\n",
      "Cost after iteration 12: 0.000076\n",
      "Cost after iteration 13: 0.000076\n",
      "Cost after iteration 14: 0.000075\n",
      "Cost after iteration 15: 0.000075\n",
      "Cost after iteration 16: 0.000074\n",
      "Cost after iteration 17: 0.000074\n",
      "Cost after iteration 18: 0.000074\n",
      "Cost after iteration 19: 0.000073\n",
      "Cost after iteration 20: 0.000073\n",
      "Cost after iteration 21: 0.000073\n",
      "Cost after iteration 22: 0.000072\n",
      "Cost after iteration 23: 0.000072\n",
      "Cost after iteration 24: 0.000071\n",
      "Cost after iteration 25: 0.000071\n",
      "Cost after iteration 26: 0.000071\n",
      "Cost after iteration 27: 0.000070\n",
      "Cost after iteration 28: 0.000070\n",
      "Cost after iteration 29: 0.000070\n",
      "Cost after iteration 30: 0.000069\n",
      "Cost after iteration 31: 0.000069\n",
      "Cost after iteration 32: 0.000069\n",
      "Cost after iteration 33: 0.000068\n",
      "Cost after iteration 34: 0.000068\n",
      "Cost after iteration 35: 0.000068\n",
      "Cost after iteration 36: 0.000067\n",
      "Cost after iteration 37: 0.000067\n",
      "Cost after iteration 38: 0.000067\n",
      "Cost after iteration 39: 0.000067\n",
      "Cost after iteration 40: 0.000066\n",
      "Cost after iteration 41: 0.000066\n",
      "Cost after iteration 42: 0.000066\n",
      "Cost after iteration 43: 0.000065\n",
      "Cost after iteration 44: 0.000065\n",
      "Cost after iteration 45: 0.000065\n",
      "Cost after iteration 46: 0.000064\n",
      "Cost after iteration 47: 0.000064\n",
      "Cost after iteration 48: 0.000064\n",
      "Cost after iteration 49: 0.000064\n",
      "Cost after iteration 50: 0.000063\n",
      "Cost after iteration 51: 0.000063\n",
      "Cost after iteration 52: 0.000063\n",
      "Cost after iteration 53: 0.000062\n",
      "Cost after iteration 54: 0.000062\n",
      "Cost after iteration 55: 0.000062\n",
      "Cost after iteration 56: 0.000062\n",
      "Cost after iteration 57: 0.000061\n",
      "Cost after iteration 58: 0.000061\n",
      "Cost after iteration 59: 0.000061\n",
      "Cost after iteration 60: 0.000061\n",
      "Cost after iteration 61: 0.000060\n",
      "Cost after iteration 62: 0.000060\n",
      "Cost after iteration 63: 0.000060\n",
      "Cost after iteration 64: 0.000060\n",
      "Cost after iteration 65: 0.000059\n",
      "Cost after iteration 66: 0.000059\n",
      "Cost after iteration 67: 0.000059\n",
      "Cost after iteration 68: 0.000059\n",
      "Cost after iteration 69: 0.000058\n",
      "Cost after iteration 70: 0.000058\n",
      "Cost after iteration 71: 0.000058\n",
      "Cost after iteration 72: 0.000058\n",
      "Cost after iteration 73: 0.000057\n",
      "Cost after iteration 74: 0.000057\n",
      "Cost after iteration 75: 0.000057\n",
      "Cost after iteration 76: 0.000057\n",
      "Cost after iteration 77: 0.000056\n",
      "Cost after iteration 78: 0.000056\n",
      "Cost after iteration 79: 0.000056\n",
      "Cost after iteration 80: 0.000056\n",
      "Cost after iteration 81: 0.000056\n",
      "Cost after iteration 82: 0.000055\n",
      "Cost after iteration 83: 0.000055\n",
      "Cost after iteration 84: 0.000055\n",
      "Cost after iteration 85: 0.000055\n",
      "Cost after iteration 86: 0.000055\n",
      "Cost after iteration 87: 0.000054\n",
      "Cost after iteration 88: 0.000054\n",
      "Cost after iteration 89: 0.000054\n",
      "Cost after iteration 90: 0.000054\n",
      "Cost after iteration 91: 0.000054\n",
      "Cost after iteration 92: 0.000053\n",
      "Cost after iteration 93: 0.000053\n",
      "Cost after iteration 94: 0.000053\n",
      "Cost after iteration 95: 0.000053\n",
      "Cost after iteration 96: 0.000053\n",
      "Cost after iteration 97: 0.000052\n",
      "Cost after iteration 98: 0.000052\n",
      "Cost after iteration 99: 0.000052\n"
     ]
    }
   ],
   "source": [
    "def neural_network(dataset, T, params, num_iterations = 100):\n",
    "    for k in range(0, num_iterations):\n",
    "        loss = 0\n",
    "        num_inputs = len(dataset)\n",
    "        for i in range(0, num_inputs):\n",
    "            X = dataset[i]\n",
    "            model = model_setup(X,T)\n",
    "            # Forward propagation.\n",
    "            fwd_result = forward_pass(model,params)\n",
    "            # Calculate loss\n",
    "            loss = calculate_loss(fwd_result[\"Y\"], T)\n",
    "            # Backward propogation\n",
    "            grads = backward_pass(model, params, fwd_result)\n",
    "            # Update weights\n",
    "            params = update_params(params, grads)\n",
    "        # Print the cost every 100 iterations\n",
    "        if i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (k, loss))\n",
    "\n",
    "neural_network([train_model.X], train_model.T , copy.deepcopy(params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The MNIST_Fashion dataset comes with Keras, and can be loaded using the commands: fashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() This dataset corresponds to grayscale coded images of various clothing items. Just as in MNIST, there are 10 classes and each image is a 28*28 matrix (see https://www.tensorflow.org/tutorials/keras/basic_classification for more information about this dataset). Answer the following:<br><ul><li>a. What is the best classification accuracy that you can achieve with the Linear Model on this dataset?</li><li>b. Replace the Linear model with a Fully Connected Feed Forward Model with one or more Hidden Layers. What is the best classification accuracy that you can achieve with this model? In order to get better performance , you can vary the number of hidden layers and the number of nodes per hidden layer</li></ul><br><br>(8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. The figure above shows an example of Ensemble Processing (see Lecture 4). Start with the best model that you have after doing Problem 3, and replicate that model 3 times in a parallel configuration as shown in the figure (use the Keras Functional API for doing this). Take the final logit nodes from each branch and average it to create the final logit layer for the model (see https://keras.io/api/layers/merging_layers/average/ on how to do this). Train the model again on the fashion_mnist dataset, and check for an improvement in prediction accuracy (if any).<img src=\"imgs/hw2-Q4.jpg\"><br><br>(5 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Answer:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
